# Inference providers
ASR_PROVIDER=faster_whisper   # faster_whisper | openai
LLM_PROVIDER=openai           # openai | local

# Faster-Whisper
FWHISPER_MODEL=tiny           # tiny | base | small | medium (CPU-friendly: tiny/base/small)

# OpenAI
OPENAI_API_KEY=YOUR_OPENAI_KEY
OPENAI_WHISPER_MODEL=whisper-1
OPENAI_GPT_MODEL=gpt-4o-mini  # cost/latency-friendly for streaming summaries

# Server
HOST=0.0.0.0
PORT=8000

# Pipeline tuning
CHUNK_SECONDS=2
ASR_OVERLAP_SECONDS=0.5
SUMMARY_INTERVAL_SECONDS=5
SESSION_TIMEOUT_SECONDS=300

# Metrics
PROMETHEUS_PORT=9000